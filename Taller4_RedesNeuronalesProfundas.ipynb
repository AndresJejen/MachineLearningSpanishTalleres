{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fWpcWHOERBNX"
   },
   "source": [
    "# Construcción de una red profunda\n",
    "\n",
    "Previamente hemos entrenado una red neuronal de dos capas (con una capa escondida), Ahora vamos a cosntruir una red profunda, con múltiples capas escondidas.\n",
    "\n",
    "- En este taller, se van a implementar las funciones requeridas para construir una red neuronal profunda.\n",
    "- Luego se podrán utilizar estas funciones para construir una red neuronal profunda para clasificación de imagenes.\n",
    "\n",
    "**Luego de este taller usted va a haber aprendido a:**\n",
    "- Utilizar unidades no-lineales mediante una función como ReLU para mejorar el modelo\n",
    "- Construir una red neuronal profunda (con más de una capa escondida)\n",
    "- Implementar de manera práctica una red neuronal\n",
    "\n",
    "**Notación**:\n",
    "- Superíndice $[l]$ denota una cantidad asociada con la $l-ésima$ capa. \n",
    "    - Ejemplo: $a^{[l]}$ es la activación de la $l-ésima$ capa. $W^{[l]}$ y $b^{[l]}$ son los parámetros de la $l-ésima$ capa.\n",
    "- Superíndice $(i)$ denota una cantidad asociada con el $i-ésimo$ ejemplo. \n",
    "    - Ejemplo: $x^{(i)}$ es el $i-ésimo$ ejemplo de entrenamiento.\n",
    "- Subíndice $i$ denota la $i-ésima$ entrada de un vector.\n",
    "    - Ejemplo: $a^{[l]}_i$ denota la $i-ésima$ entrada de las activaciones de la $l-ésima$ capa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3E3WjkbORBNZ"
   },
   "source": [
    "## 1. Paquetes\n",
    "\n",
    "Primero se deben importar todos los paquetes que se van a necesitar durante este taller.\n",
    "- [numpy](www.numpy.org) paquete básico para ciencias computacionales con Python.\n",
    "- [matplotlib](http://matplotlib.org) librería para graficar en Python.\n",
    "- dnn_utils provee distintas funciones que se van a usar durante el taller\n",
    "- testCases tiene los ejemplos de prueba para evaluar la implementacion de las funciones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 389
    },
    "colab_type": "code",
    "id": "aUIl54DrRBNb",
    "outputId": "d40c28aa-93e1-41fd-fdc8-7e2f8f12941d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from testCases_v4 import *\n",
    "from dnn_utils_v2 import sigmoid, sigmoid_backward, relu, relu_backward\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # se fija el tamaño de los gráficos\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)     # se utiliza para replicar las funciones aleatorias "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D-NVSwn-RBNj"
   },
   "source": [
    "## 2 - Resumen del taller\n",
    "\n",
    "Para construir la red neuronal, primero se construirán funciones auxiliares que permitirán implementar una red neuronal de dos y de L capas. Cada función auxiliar se puede construir siguiendo las instrucciones: \n",
    "\n",
    "- Inicializar los parámetros para una red de 2 capas y para una red de $L$ capas.\n",
    "- Implementar la propagación hacia delante.\n",
    "     - Desarrolle la propagación LINEAL hacia delante de una capa, obteniendo $Z^{[l]}$, y luego la función de ACTIVACION, RELU o SIGMOIDE.\n",
    "     - Combine los pasos [LINEAL->ACTIVACION] en una sola función (hacia delante).\n",
    "     - Agrupe las funciones de propagación hacia delante [LINEAL->RELU] L-1 veces (para las capas 1 hasta L-1) y añada una [LINEAL->SIGMOIDE] al final (para la última capa $L$). De esta manera obtendrá la función L_model_forward.\n",
    "- Compute la pérdida.\n",
    "- Implemente la retro-propagación.\n",
    "    - Complete la parte LINEAL de la retro-propagación de una capa (el gradiente de la función de activación le va a ser proporcionado). \n",
    "    - Combine los pasos en una nueva función de retro-propagación [LINEAL->ACTIVACION].\n",
    "    - Agrupe la funciones de retro-propagación [LINEAL->RELU] L-1 veces y añada la correspondiente [LINEAL->SIGMOIDE] en una nueva función L_model_backward.\n",
    "- Por último, actualice los parámetros.\n",
    "\n",
    "**Anotación:** cada función de propagación hacia delante tiene su correspondiente función hacia atrás. Por ello, a cada paso hacia delante, se guardan en la caché algunos valores necesarios para calcular los gradientes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cpHSWV4FRBNl"
   },
   "source": [
    "## 3 - Inicialización\n",
    "\n",
    "Desarrolle dos funciones auxiliares para inicializar los parámetros de su modelo. La primera función permitirá inicializar los parámetros para un modelo con dos capas. La segunda permitirá generalizar el proceso de inicialización para $L$ capas.\n",
    "\n",
    "### 3.1 - Red neuronal con 2 capas\n",
    "\n",
    "**Ejercicio**: Crear e inicializar los parámetros para una red de 2 capas.\n",
    "\n",
    "**Instrucciones**:\n",
    "- La estructura del modelo es *LINEAL -> RELU -> LINEAL -> SIGMOIDE*. \n",
    "- Inicializar aleatoriamente los pesos. Puede utilizar la función `np.random.randn(dimensiones)*0.01` con las dimensiones correctas.\n",
    "- Inicialice los sesgos a cero. Puede utilizar la función `np.zeros(dimensiones)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "THVSOe0TRBNo"
   },
   "outputs": [],
   "source": [
    "# FUNCIÓN A CALIFICAR: initialize_parameters\n",
    "\n",
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    n_x: tamaño de la capa de entrada\n",
    "    n_h: tamaño de la capa escondida\n",
    "    n_y: tamaño de la capa de salida\n",
    "    Output:\n",
    "    parameters: diccionario python con los parametros parameters:\n",
    "                    W1: matriz de pesos con dimensiones (n_h, n_x)\n",
    "                    b1: matriz de sesgos con dimensiones (n_h, 1)\n",
    "                    W2: matriz de pesos con dimensiones (n_y, n_h)\n",
    "                    b2: matriz de sesgos con dimensiones (n_y, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    \n",
    "    ### EMPIEZE EL CÓDIGO AQUÍ ### (≈ 4 líneas de código)\n",
    "    W1 = \n",
    "    b1 = \n",
    "    W2 = \n",
    "    b2 =\n",
    "    ### TERMINE EL CÓDIGO AQUÍ ###\n",
    "    \n",
    "    assert(W1.shape == (n_h, n_x))\n",
    "    assert(b1.shape == (n_h, 1))\n",
    "    assert(W2.shape == (n_y, n_h))\n",
    "    assert(b2.shape == (n_y, 1))\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iUgyaksPRBNu",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parameters = initialize_parameters(3,2,1)\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G1Lp_ICHRBN2"
   },
   "source": [
    "**Salida esperada**:\n",
    "       \n",
    "<table style=\"width:80%\">\n",
    "  <tr>\n",
    "    <td> **W1** </td>\n",
    "    <td> [[ 0.01624345 -0.00611756 -0.00528172]\n",
    " [-0.01072969  0.00865408 -0.02301539]] </td> \n",
    "  </tr>\n",
    "\n",
    "  <tr>\n",
    "    <td> **b1**</td>\n",
    "    <td>[[ 0.]\n",
    " [ 0.]]</td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>**W2**</td>\n",
    "    <td> [[ 0.01744812 -0.00761207]]</td>\n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td> **b2** </td>\n",
    "    <td> [[ 0.]] </td> \n",
    "  </tr>\n",
    "  \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "triIbodZRBN2"
   },
   "source": [
    "### 3.2 - Red Neuronal con L capas\n",
    "\n",
    "La inicialización de una red neuronal profunda es más compleja al haber más matrices de pesos y vectores de sesgo. Al completar `initialize_parameters_deep`, debe asegurarse que sus dimensiones sean coherentes al pasar de capa en capa. Recuerde que  $n^{[l]}$ es el número de unidades en la capa $l$. Entonces, e.g., si el tamaño de la entrada $X$ es $(12288, 209)$ (con $m=209$ ejemplos), se tiene que:\n",
    "\n",
    "<table style=\"width:100%\">\n",
    "\n",
    "\n",
    "    <tr>\n",
    "        <td>  </td> \n",
    "        <td> **Dimensión de W** </td> \n",
    "        <td> **Dimensión de b**  </td> \n",
    "        <td> **Activación** </td>\n",
    "        <td> **Dimensión de la activación** </td> \n",
    "    <tr>\n",
    "    \n",
    "    <tr>\n",
    "        <td> **Capa 1** </td> \n",
    "        <td> $(n^{[1]},12288)$ </td> \n",
    "        <td> $(n^{[1]},1)$ </td> \n",
    "        <td> $Z^{[1]} = W^{[1]}  X + b^{[1]} $ </td> \n",
    "        \n",
    "        <td> $(n^{[1]},209)$ </td> \n",
    "    <tr>\n",
    "    \n",
    "    <tr>\n",
    "        <td> **Capa 2** </td> \n",
    "        <td> $(n^{[2]}, n^{[1]})$  </td> \n",
    "        <td> $(n^{[2]},1)$ </td> \n",
    "        <td>$Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]}$ </td> \n",
    "        <td> $(n^{[2]}, 209)$ </td> \n",
    "    <tr>\n",
    "   \n",
    "       <tr>\n",
    "        <td> $\\vdots$ </td> \n",
    "        <td> $\\vdots$  </td> \n",
    "        <td> $\\vdots$  </td> \n",
    "        <td> $\\vdots$</td> \n",
    "        <td> $\\vdots$  </td> \n",
    "    <tr>\n",
    "    \n",
    "   <tr>\n",
    "        <td> **Capa L-1** </td> \n",
    "        <td> $(n^{[L-1]}, n^{[L-2]})$ </td> \n",
    "        <td> $(n^{[L-1]}, 1)$  </td> \n",
    "        <td>$Z^{[L-1]} =  W^{[L-1]} A^{[L-2]} + b^{[L-1]}$ </td> \n",
    "        <td> $(n^{[L-1]}, 209)$ </td> \n",
    "    <tr>\n",
    "    \n",
    "    \n",
    "   <tr>\n",
    "        <td> **Capa L** </td> \n",
    "        <td> $(n^{[L]}, n^{[L-1]})$ </td> \n",
    "        <td> $(n^{[L]}, 1)$ </td>\n",
    "        <td> $Z^{[L]} =  W^{[L]} A^{[L-1]} + b^{[L]}$</td>\n",
    "        <td> $(n^{[L]}, 209)$  </td> \n",
    "    <tr>\n",
    "\n",
    "</table>\n",
    "\n",
    "Recuerde que en python, el cálculo de $W X + b$ lleva a cabo la operación de broadcasting.\n",
    "\n",
    "Entonces, si:\n",
    "\n",
    "$$ W = \\begin{bmatrix}\n",
    "    j  & k  & l\\\\\n",
    "    m  & n & o \\\\\n",
    "    p  & q & r \n",
    "\\end{bmatrix}\\;\\;\\; X = \\begin{bmatrix}\n",
    "    a  & b  & c\\\\\n",
    "    d  & e & f \\\\\n",
    "    g  & h & i \n",
    "\\end{bmatrix} \\;\\;\\; b =\\begin{bmatrix}\n",
    "    s  \\\\\n",
    "    t  \\\\\n",
    "    u\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "el resultado de $WX + b$ será:\n",
    "\n",
    "$$ WX + b = \\begin{bmatrix}\n",
    "    (ja + kd + lg) + s  & (jb + ke + lh) + s  & (jc + kf + li)+ s\\\\\n",
    "    (ma + nd + og) + t & (mb + ne + oh) + t & (mc + nf + oi) + t\\\\\n",
    "    (pa + qd + rg) + u & (pb + qe + rh) + u & (pc + qf + ri)+ u\n",
    "\\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EgkyRm-jRBN6"
   },
   "source": [
    "**Ejercicio**: Implemente la inicialización de una red neuronal con L capas. \n",
    "\n",
    "**Instrucciones**:\n",
    "- La estructura del modelo es *[LINEAL -> RELU] $ \\times$ (L-1) -> LINEAL -> SIGMOIDE*. Esto es, la red tiene $L-1$ capas utilizando una función de activación ReLU, seguido de una capa de salida con la función de activación Sigmoide.\n",
    "- Use una inicialización aleatoria para las matrices de pesos. Utilice `np.random.randn(shape) * 0.01`.\n",
    "- Use una inicialización de ceros para los sesgos. Utilice `np.zeros(shape)`.\n",
    "- El número de unidades en cada capa $n^{[l]}$, se guarda en la variable `layer_dims`. De esta manera, por ejemplo, las `layer_dims` para el taller de la semana pasada con \"Un red neuronal sencilla\" sería [2,4,1]: donde hay 2 entradas, una capa escondida con 4 unidades, y una capa de salida con una unidad. Por lo tanto, la forma de `W1` es de (4,2), la de `b1` (4,1), `W2` (1,4) y `b2` (1,1). Ahora se puede generalizar para $L$ capas! \n",
    "\n",
    "\n",
    "**Ayuda**:\n",
    "La implementación para $L=1$ sería de la siguiente manera:\n",
    "```python\n",
    "    if L == 1:\n",
    "        parameters[\"W\" + str(L)] = np.random.randn(layer_dims[1], layer_dims[0]) * 0.01\n",
    "        parameters[\"b\" + str(L)] = np.zeros((layer_dims[1], 1))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "datFEzFPRBN-"
   },
   "outputs": [],
   "source": [
    "# FUNCIÓN A CALIFICAR: initialize_parameters_deep\n",
    "\n",
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    layer_dims: arreglo (lista) de python con las dimensiones de cada capa de la red\n",
    "    Output:\n",
    "    parameters: diccionario python con los parametros \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl: matriz de pesos (layer_dims[l], layer_dims[l-1])\n",
    "                    bl: vector de sesgo (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            # número de capas de la red\n",
    "\n",
    "    for l in range(1, L):\n",
    "        ### EMPIEZE EL CÓDIGO AQUÍ ### (≈ 2 líneas de código)\n",
    "        parameters['W' + str(l)] = \n",
    "        parameters['b' + str(l)] = \n",
    "        ### TERMINE EL CÓDIGO AQUÍ ###\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s3uVWRL_RBOG"
   },
   "outputs": [],
   "source": [
    "parameters = initialize_parameters_deep([5,4,3])\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r-TIPQMiRBOL"
   },
   "source": [
    "**Salida esperada**:\n",
    "       \n",
    "<table style=\"width:80%\">\n",
    "  <tr>\n",
    "    <td> **W1** </td>\n",
    "    <td>[[ 0.01788628  0.0043651   0.00096497 -0.01863493 -0.00277388]\n",
    " [-0.00354759 -0.00082741 -0.00627001 -0.00043818 -0.00477218]\n",
    " [-0.01313865  0.00884622  0.00881318  0.01709573  0.00050034]\n",
    " [-0.00404677 -0.0054536  -0.01546477  0.00982367 -0.01101068]]</td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>**b1** </td>\n",
    "    <td>[[ 0.]\n",
    " [ 0.]\n",
    " [ 0.]\n",
    " [ 0.]]</td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>**W2** </td>\n",
    "    <td>[[-0.01185047 -0.0020565   0.01486148  0.00236716]\n",
    " [-0.01023785 -0.00712993  0.00625245 -0.00160513]\n",
    " [-0.00768836 -0.00230031  0.00745056  0.01976111]]</td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>**b2** </td>\n",
    "    <td>[[ 0.]\n",
    " [ 0.]\n",
    " [ 0.]]</td> \n",
    "  </tr>\n",
    "  \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b0ln0Oo1RBOQ"
   },
   "source": [
    "## 4 - Propagación hacia delante\n",
    "\n",
    "### 4.1 - Lineal hacia delante \n",
    "Una vez inicializados los parámetros, debe implementar la propagación hacia delante. Va a empezar por implementar algunas funciones básicas para ser utilizadas más adelante en la implementación del modelo. Va a implementar 3 funciones:\n",
    "\n",
    "- LINEAL\n",
    "- LINEAL -> ACTIVACION donde la activación será ReLU o Sigmoide. \n",
    "- [LINEAL -> RELU] $\\times$ (L-1) -> LINEAL -> SIGMOIDE (modelo completo)\n",
    "\n",
    "Esta implementación (vectorizada) de la propagación hacia delante calcula las siguientes ecuaciones:\n",
    "\n",
    "$$Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}$$\n",
    "\n",
    "donde $A^{[0]} = X$. \n",
    "\n",
    "**Ejercicio**: Construya la parte LINEAL de la propagación hacia delante.\n",
    "\n",
    "**Ayuda**:\n",
    "La representación matemática de esta implementación para una capa es $Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}$. Puede ser útil la función`np.dot()`. También, si las dimensiones no casan, puede investigar lo que ocurre llamando a `W.shape`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "njh31EP2RBOR"
   },
   "outputs": [],
   "source": [
    "# FUNCIÓN A CALIFICAR: linear_forward\n",
    "\n",
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Implemente la parte lineal para la propagación hacia delante de una capa.\n",
    "    Input:\n",
    "    A: las activaciones de la capa previa (o los datos de entrada): (tamaño de la capa previa, número de ejemplos)\n",
    "    W: matriz de pesos, un arreglo numpy de dimensiones (tamaño de la capa actual, tamaño de la capa previa)\n",
    "    b: vector de sesgo, un arreglo numpy de dimensiones (tamaño de la capa actual, 1)\n",
    "    Output:\n",
    "    Z: la entrada para la función de activación, también llamado parámetro de pre-activación \n",
    "    cache: diccionario python con \"A\", \"W\" y \"b\", almacenados para computar los pasos hacia atrás de manera eficiente\n",
    "    \"\"\"\n",
    "    \n",
    "    ### EMPIEZE EL CÓDIGO AQUÍ ### (≈ 1 línea de código)\n",
    "    Z = \n",
    "    ### TERMINE EL CÓDIGO AQUÍ ###\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yBGqCS-ERBOZ"
   },
   "outputs": [],
   "source": [
    "A, W, b = linear_forward_test_case()\n",
    "\n",
    "Z, linear_cache = linear_forward(A, W, b)\n",
    "print(\"Z = \" + str(Z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ttv10rmvRBOd"
   },
   "source": [
    "**Salida esperada**:\n",
    "\n",
    "<table style=\"width:35%\">\n",
    "  \n",
    "  <tr>\n",
    "    <td> **Z** </td>\n",
    "    <td> [[ 3.26295337 -1.23429987]] </td> \n",
    "  </tr>\n",
    "  \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xp5NKQDrRBOf"
   },
   "source": [
    "### 4.2 - Activación-lineal hacia delante\n",
    "\n",
    "En este taller, vamos a utilizar dos funciones de activación:\n",
    "\n",
    "- **Sigmoide**: $\\sigma(Z) = \\sigma(W A + b) = \\frac{1}{ 1 + e^{-(W A + b)}}$. Esta función `sigmoid` está dada, y devuelve 2 objetos: el valor de activación \"`a`\" y una \"`cache`\" que contiene \"`Z`\" (la cual se le pasa a la correspondiente función de retro-propagación). \n",
    "\n",
    "Para usarla basta con este comando: \n",
    "``` python\n",
    "A, activation_cache = sigmoid(Z)\n",
    "```\n",
    "\n",
    "- **ReLU**: La fórmula matemática para ReLU es $A = RELU(Z) = max(0, Z)$. Esta función `relu` también está dada, y devuelve 2 objetos: el valor de activación \"`a`\" y una \"`cache`\" que contiene \"`Z`\" (la cual se le pasa a la correspondiente función de retro-propagación). \n",
    "\n",
    "Para usarla basta con este comando: \n",
    "``` python\n",
    "A, activation_cache = relu(Z)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DagPoL5lRBOg"
   },
   "source": [
    "Para mayor conveniencia, vamos a agrupar dos funciones (Lineal y Activacion) en una sola (LINEAL->ACTIVACION). Por lo tanto, va a implementar una función que da el paso LINEAL hacia delante seguido del paso de ACTIVACION hacia delante.\n",
    "\n",
    "**Ejercicio**: Implemente la propagación hacia delante de la capa *LINEAL->ACTIVACION*. La ecuación matemática es: $$A^{[l]} = g(Z^{[l]}) = g(W^{[l]}A^{[l-1]} +b^{[l]})$$ donde la activación \"g\" puede ser sigmoide() o relu(). Utilice linear_forward() y la función de activación correcta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pI79XgKSRBOg"
   },
   "outputs": [],
   "source": [
    "# FUNCIÓN A CALIFICAR: linear_activation_forward\n",
    "\n",
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implemente la propagación hacia delante para la capa LINEAL->ACTIVACION\n",
    "    Input:\n",
    "    A_prev: activaciones de la capa previa (o de los datos de entrada): (tamaño de la capa previa, número de ejemplos)\n",
    "    W: matriz de pesos, un arreglo numpy de dimensiones (tamaño de la capa actual, tamaño de la capa previa)\n",
    "    b: vector de sesgo, un arreglo numpy de dimensiones (tamaño de la capa actual, 1)\n",
    "    activation: la activación a ser usada en la capa, guardada como una cadena de texto: \"sigmoid\" or \"relu\"\n",
    "    Output:\n",
    "    A: la salida de la función de activación, también llamada valor de post-activacion \n",
    "    cache: dicionario python con la \"cache_lineal\" y la \"cache_activacion\"\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        ### EMPIEZE EL CÓDIGO AQUÍ ### (≈ 2 líneas de código)\n",
    "        Z, linear_cache = \n",
    "        A, activation_cache = \n",
    "        ### TERMINE EL CÓDIGO AQUÍ ###\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        ### EMPIEZE EL CÓDIGO AQUÍ ### (≈ 2 líneas de código)\n",
    "        Z, linear_cache = \n",
    "        A, activation_cache = \n",
    "        ### TERMINE EL CÓDIGO AQUÍ ###\n",
    "    \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j9ALoVBLRBOj"
   },
   "outputs": [],
   "source": [
    "A_prev, W, b = linear_activation_forward_test_case()\n",
    "\n",
    "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"sigmoid\")\n",
    "print(\"Con sigmoide: A = \" + str(A))\n",
    "\n",
    "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"relu\")\n",
    "print(\"Con ReLU: A = \" + str(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j0EjBNSBRBOp"
   },
   "source": [
    "**Salida esperada**:\n",
    "       \n",
    "<table style=\"width:35%\">\n",
    "  <tr>\n",
    "    <td> **With sigmoid: A ** </td>\n",
    "    <td > [[ 0.96890023  0.11013289]]</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> **With ReLU: A ** </td>\n",
    "    <td > [[ 3.43896131  0.        ]]</td> \n",
    "  </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FoH6JrMLRBOq"
   },
   "source": [
    "**Nota**: En deep learning, la computación de \"[LINEAL->ACTIVACION]\" se cuenta como una sola capa de la red neuronal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CdQINoZuRBOr"
   },
   "source": [
    "### d) Modelo con L capas \n",
    "\n",
    "Con el fin de facilitar la implementación de la red neuronal de $L$ capas que queremos implementar, necesitamos una función que replique la propagación hacia delante (`linear_activation_forward`) con RELU, $L-1$ veces, seguida por la función (`linear_activation_forward`) SIGMOIDE.\n",
    "\n",
    "\n",
    "**Ejercicio**: Implemente la propagación hacia delante del modelo descrito anteriormente.\n",
    "\n",
    "**Instrucciones**: En el código abajo, la variable `AL` denota $A^{[L]} = \\sigma(Z^{[L]}) = \\sigma(W^{[L]} A^{[L-1]} + b^{[L]})$ (Esta es la estimación $\\hat{Y}$.) \n",
    "\n",
    "**Ayuda**:\n",
    "- Use las funciones que ha programada arriba \n",
    "- Use un bucle for para replicar la [LINEAL->RELU] (L-1) veces\n",
    "- No olvide ir guardando las caches en la lista \"caches\". Para añadir un nuevo valor use `list.append(c)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FpN7LGUBRBOr"
   },
   "outputs": [],
   "source": [
    "# FUNCIÓN A CALIFICAR: L_model_forward\n",
    "\n",
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Implemente la propagación hacia delante para calcular [LINEAL->RELU]*(L-1)->LINEAL->SIGMOIDE\n",
    "    Input:\n",
    "    X: datos de entrada, arreglo de tamaño (tamaño del input, número de ejemplos)\n",
    "    parameters: salida de initialize_parameters_deep()\n",
    "    Output:\n",
    "    AL: último valor de post-activación\n",
    "    caches: lista de caches con cada caché de linear_activation_forward() (hay L-1 cachés, indexadas de 0 a L-1)\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                  # número de capas en la red neuronal\n",
    "    \n",
    "    # Implemente [LINEAL -> RELU]*(L-1). Añada \"cache\" a la lista de \"caches\".\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        ### EMPIEZE EL CÓDIGO AQUÍ ### (≈ 2 líneas de código)\n",
    "        A, cache = \n",
    "                        \n",
    "        ### TERMINE EL CÓDIGO AQUÍ ###\n",
    "    \n",
    "    # Implemente LINEAL -> SIGMOIDE. Añada \"cache\" a la lista de \"caches\".\n",
    "    ### EMPIEZE EL CÓDIGO AQUÍ ### (≈ 2 líneas de código)\n",
    "    AL, cache =\n",
    "    \n",
    "    ### TERMINE EL CÓDIGO AQUÍ ###\n",
    "    \n",
    "    assert(AL.shape == (1,X.shape[1]))\n",
    "            \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2_9QNstiRBOv"
   },
   "outputs": [],
   "source": [
    "X, parameters = L_model_forward_test_case_2hidden()\n",
    "AL, caches = L_model_forward(X, parameters)\n",
    "print(\"AL = \" + str(AL))\n",
    "print(\"Longitud de la lista de caches = \" + str(len(caches)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yHeX5gXoRBOy"
   },
   "source": [
    "**Salida esperada**:\n",
    "\n",
    "<table style=\"width:50%\">\n",
    "  <tr>\n",
    "    <td> **AL** </td>\n",
    "    <td > [[ 0.03921668  0.70498921  0.19734387  0.04728177]]</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> **Longitud de la lista de caches ** </td>\n",
    "    <td > 3 </td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U6iOjfkMRBO0"
   },
   "source": [
    "Muy bien, llegado a este punto ya tiene todo el proceso de propagación hacia delente completo, tomando el input X y obteniendo outputs del vector-fila $A^{[L]}$ con sus predicciones (a partir de lo cual puede calcular el coste o pérdida de sus predicciones). También se ha quedado con los valores intermedios en \"caches\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MoLP8-WNRBO1"
   },
   "source": [
    "## 5 - Función de pérdida o coste\n",
    "\n",
    "Ahora va a implementar la propagación hacia delate y hacia atrás. Debe computar el coste con el fin de verificar si su modelo en verdad está aprendiendo.\n",
    "\n",
    "**Ejercicio**: Calcule el coste por entropía-cruzada $J$, en base a la siguiente fórmula: $$-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right)) $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DVAIX9JnRBO4"
   },
   "outputs": [],
   "source": [
    "# FUNCIÓN A CALIFICAR: compute_cost\n",
    "\n",
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Implemente la función de coste por entrpía cruzada.\n",
    "    Input:\n",
    "    AL: vector con las probabilidades para las etiquetas de predicción, dimensiones (1, número de ejemplos)\n",
    "    Y: vector de etiquetas observadas, de dimensión (1, número de ejemplos)\n",
    "    Output:\n",
    "    coste: coste de entropía cruzada\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "\n",
    "    # Compute la pérdida de AL e Y.\n",
    "    ### EMPIEZE EL CÓDIGO AQUÍ ### (≈ 1 línea de código)\n",
    "    cost = \n",
    "    ### TERMINE EL CÓDIGO AQUÍ ###\n",
    "    \n",
    "    cost = np.squeeze(cost)      # Para asegurar que la dimensión de se coste es correcta (e.g. [[17]] se torna en 17).\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TCz8EVRERBO8"
   },
   "outputs": [],
   "source": [
    "Y, AL = compute_cost_test_case()\n",
    "\n",
    "print(\"coste = \" + str(compute_cost(AL, Y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZSxvU9Q8RBPB"
   },
   "source": [
    "**Salida esperada**:\n",
    "\n",
    "<table>\n",
    "\n",
    "    <tr>\n",
    "    <td>**coste** </td>\n",
    "    <td> 0.41493159961539694</td> \n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "85fYtZGCRBPB"
   },
   "source": [
    "## 6 - Retro-propagación\n",
    "\n",
    "Como en el caso de la propagación hacia delante, va a implementar funciones auxiliares para la retro-propagación. Recuerde que la retro-propagación permite calcular el gradiente de la función de coste con respecto a los parámetros. \n",
    "\n",
    "Análogamente a la propgación hacia delante, la retro-propagación se va a construir en tres pasos:\n",
    "- LINEAL hacia atrás\n",
    "- LINEAL -> ACTIVACION hacia atrás, donde ACTIVACION calcula la derivada de la función de activación (ReLU o sigmoide)\n",
    "- [LINEAL -> RELU] $\\times$ (L-1) -> LINEAL -> SIGMOIDE hacia atrás (modelo completo)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9546YENJRBPD"
   },
   "source": [
    "### 6.1 - Linear hacia atrás\n",
    "\n",
    "Para la capa $l$, la parte lineal es: $Z^{[l]} = W^{[l]} A^{[l-1]} + b^{[l]}$ (seguida por una activación).\n",
    "\n",
    "Suponga que ya ha calculado la derivada $dZ^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial Z^{[l]}}$. Ahora quiere obtener $(dW^{[l]}, db^{[l]} dA^{[l-1]})$.\n",
    "\n",
    "\n",
    "Los tres outputs $(dW^{[l]}, db^{[l]}, dA^{[l]})$ son computados utilizando el input $dZ^{[l]}$. \n",
    "\n",
    "Estas son la fórmulas que necesita:\n",
    "$$ dW^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T} $$\n",
    "$$ db^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}$$\n",
    "$$ dA^{[l-1]} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-6nGhER6RBPF"
   },
   "source": [
    "**Ejercicio**: Utilice las tres fórmulas (arriba) para implementar linear_backward()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fPJK3v1iRBPG"
   },
   "outputs": [],
   "source": [
    "# FUNCIÓN A CALIFICAR: linear_backward\n",
    "\n",
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implemente la parte lineal de la retro-propagación para una sola capa [l]\n",
    "    Input:\n",
    "    dZ: Gradiente del coste con respecto al output lineal de la capa actual\n",
    "    cache: conjunto de velores (A_prev, W, b) provenientes de la propagación hacia delante en la capa actual\n",
    "    Output:\n",
    "    dA_prev: Gradiente del coste con respecto a la activación (de la capa previa: l-1), del mismo tamaño como A_prev\n",
    "    dW: Gradiente del coste con respecto a W (de la capa actual: l), del mismo tamaño que W\n",
    "    db: Gradiente del coste con respecto a b (de la capa actual: l), del mismo tamaño que b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    ### EMPIEZE EL CÓDIGO AQUÍ ### (≈ 3 líneas de código)\n",
    "    dW = \n",
    "    db = \n",
    "    dA_prev = \n",
    "    ### TERMINE EL CÓDIGO AQUÍ ###\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gAS2pP5mRBPN"
   },
   "outputs": [],
   "source": [
    "dZ, linear_cache = linear_backward_test_case()\n",
    "\n",
    "dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NdFBtVG8RBPS"
   },
   "source": [
    "**Salida esperada**:\n",
    "\n",
    "<table style=\"width:90%\">\n",
    "  <tr>\n",
    "    <td> **dA_prev** </td>\n",
    "    <td > [[ 0.51822968 -0.19517421]\n",
    " [-0.40506361  0.15255393]\n",
    " [ 2.37496825 -0.89445391]] </td> \n",
    "  </tr> \n",
    "  \n",
    "    <tr>\n",
    "        <td> **dW** </td>\n",
    "        <td > [[-0.10076895  1.40685096  1.64992505]] </td> \n",
    "    </tr> \n",
    "  \n",
    "    <tr>\n",
    "        <td> **db** </td>\n",
    "        <td> [[ 0.50629448]] </td> \n",
    "    </tr> \n",
    "    \n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N7Pgjf5KRBPS"
   },
   "source": [
    "### 6.2 - Activación-lineal hacia atrás\n",
    "\n",
    "A continuación, va a crear una función que combine la dos funciones auxiliares: **`linear_backward`** y el paso hacia atrás de la activación **`linear_activation_backward`**. \n",
    "\n",
    "Para implementar `linear_activation_backward`, se provee de dos funciones hacia atrás:\n",
    "- **`sigmoid_backward`**: Implementa retro-propagación para la unidad SIGMOIDE, tal que \n",
    "\n",
    "```python\n",
    "dZ = sigmoid_backward(dA, activation_cache)\n",
    "```\n",
    "\n",
    "- **`relu_backward`**: Implementa retro-propagación para la unidad RELU, de forma que \n",
    "\n",
    "```python\n",
    "dZ = relu_backward(dA, activation_cache)\n",
    "```\n",
    "\n",
    "Si $g(.)$ es la función de activación, entonces \n",
    "`sigmoid_backward` y `relu_backward` calculan $$dZ^{[l]} = dA^{[l]} * g'(Z^{[l]}) \\tag{11}$$.  \n",
    "\n",
    "**Ejercicio**: Implemente la retro-propagación para la capa *LINEAL->ACTIVACION*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wE3-q2CbRBPT"
   },
   "outputs": [],
   "source": [
    "# FUNCIÓN A CALIFICAR: linear_activation_backward\n",
    "\n",
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implemente la retro-propagación par la capa LINEAL->ACTIVACION .\n",
    "    Input:\n",
    "    dA: gradiente post-activacion para la capa actual l \n",
    "    cache: conjunto de valores (linear_cache, activation_cache) que se guardan para calcular la retro-propagación de manera eficiente\n",
    "    activacion: la activación a ser usada en esta capa, guardada como un arreglo de texto: \"sigmoid\" o \"relu\"\n",
    "    Output:\n",
    "    dA_prev: gradiente del coste con respecto a la activación (de la capa previa l-1), de las mismas dimensiones que A_prev\n",
    "    dW: gradiente del coste con respecto a W (capa actual l), mismas dimensiones que W\n",
    "    db: gradiente del coste con respecto a b (capa actual l), mismas dimensiones que b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        ### EMPIEZE EL CÓDIGO AQUÍ ### (≈ 2 líneas de código)\n",
    "        dZ = \n",
    "        dA_prev, dW, db = \n",
    "        ### TERMINE EL CÓDIGO AQUÍ ###\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        ### EMPIEZE EL CÓDIGO AQUÍ ### (≈ 2 líneas de código)\n",
    "        dZ = \n",
    "        dA_prev, dW, db = \n",
    "        ### TERMINE EL CÓDIGO AQUÍ ###\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GBxaCm0MRBPX"
   },
   "outputs": [],
   "source": [
    "dAL, linear_activation_cache = linear_activation_backward_test_case()\n",
    "\n",
    "dA_prev, dW, db = linear_activation_backward(dAL, linear_activation_cache, activation = \"sigmoid\")\n",
    "print (\"sigmoid:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db) + \"\\n\")\n",
    "\n",
    "dA_prev, dW, db = linear_activation_backward(dAL, linear_activation_cache, activation = \"relu\")\n",
    "print (\"relu:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "63uMR6dmRBPZ"
   },
   "source": [
    "**Salida esperada con el sigmoide:**\n",
    "\n",
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <td > dA_prev </td> \n",
    "           <td >[[ 0.11017994  0.01105339]\n",
    " [ 0.09466817  0.00949723]\n",
    " [-0.05743092 -0.00576154]] </td> \n",
    "\n",
    "  </tr> \n",
    "  \n",
    "    <tr>\n",
    "    <td > dW </td> \n",
    "           <td > [[ 0.10266786  0.09778551 -0.01968084]] </td> \n",
    "  </tr> \n",
    "  \n",
    "    <tr>\n",
    "    <td > db </td> \n",
    "           <td > [[-0.05729622]] </td> \n",
    "  </tr> \n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WMp5axMYRBPb"
   },
   "source": [
    "**Salida esperada con RELU:**\n",
    "\n",
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <td > dA_prev </td> \n",
    "           <td > [[ 0.44090989  0.        ]\n",
    " [ 0.37883606  0.        ]\n",
    " [-0.2298228   0.        ]] </td> \n",
    "\n",
    "  </tr> \n",
    "  \n",
    "    <tr>\n",
    "    <td > dW </td> \n",
    "           <td > [[ 0.44513824  0.37371418 -0.10478989]] </td> \n",
    "  </tr> \n",
    "  \n",
    "    <tr>\n",
    "    <td > db </td> \n",
    "           <td > [[-0.20837892]] </td> \n",
    "  </tr> \n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4PXh84ejRBPc"
   },
   "source": [
    "### 6.3 - Retro-propagación en L capas \n",
    "\n",
    "Ahora va a implementar la función de retro-propagación para toda la red neuronal. Recuerde que cuando implementó la función `L_model_forward`, en cada iteración, guardó una caché que contenía (X,W,b, z). En el paso e la retro-propagación, esas variables están a disposición para calcular los gradientes. Por lo tanto, en la función `L_model_backward`, se puede iterar sobre todas las capas escondidas hacia atrás, empezando de la última capa $L$. \n",
    "\n",
    "En cada paso hacia atrás, se utilizan los valores de la caché en la capa $l$, para retro-propagar sobre la capa $l$. \n",
    "\n",
    "** Inicializando la retro-propagación**:\n",
    "Para retro-propagar sobre esta red, sabemos que la salida es, \n",
    "$A^{[L]} = \\sigma(Z^{[L]})$. Por lo tanto, necesita calcular `dAL` $= \\frac{\\partial \\mathcal{L}}{\\partial A^{[L]}}$.\n",
    "Para hacerlo, utilice la fórmula:\n",
    "```python\n",
    "dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL)) # derivada del coste con respecto a AL\n",
    "```\n",
    "\n",
    "Puede utilizar el gradiente de post-activacion `dAL` para seguir yendo hacia atrás. Este gradiente se le puede pasar la función hacia atrás LINEAL->SIGMOIDE implementada antes (que utilizarán los valores guaradados por la función L_model_forward). Luego se debe utilizar un bucle `for` para iterar sobre todas las otras capas utilizando la función hacia atrás LINEAL->RELU. \n",
    "\n",
    "Se debe guardar cada dA, dW, y db en el diccionario grads. Para hacerlo, utilice la fórmula : \n",
    "\n",
    "$$grads[\"dW\" + str(l)] = dW^{[l]}$$\n",
    "\n",
    "Por ejemplo, para $l=3$ se guardaría $dW^{[l]}$ en `grads[\"dW3\"]`.\n",
    "\n",
    "**Ejercicio**: Implemente la retro-propagación para el modelo *[LINEAL->RELU] $\\times$ (L-1) -> LINEAL -> SIGMOIDE*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LyO467SJRBPd"
   },
   "outputs": [],
   "source": [
    "# FUNCIÓN A CALIFICAR: L_model_backward\n",
    "\n",
    "def L_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "    Input:\n",
    "    AL: vector con las probabilidades, salida para propagación hacia delante L_model_forward()\n",
    "    Y: vector de clases/etiquetas observadas, de dimensión (1, número de ejemplos)\n",
    "    caches: lista de caches, donde se tiene\n",
    "                - cada cache de linear_activation_forward() con \"relu\" (i.e., caches[l]; l = 0...L-2)\n",
    "                - el cache de linear_activation_forward() con \"sigmoid\" (i.e, [L-1])\n",
    "    Output:\n",
    "    grads: Un diccionario con los gradientes\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # número de capas\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # Y es del mismo tamaño que AL\n",
    "    \n",
    "    # Initializacion de la retro-propagación\n",
    "    ### EMPIEZE EL CÓDIGO AQUÍ ### (1 línea de código)\n",
    "    dAL =  # derivada del coste con respecto a AL\n",
    "    ### TERMINE EL CÓDIGO AQUÍ ###\n",
    "    \n",
    "    # Gradientes para la ultima capa L (SIGMOIDE -> LINEAL). Inputs: \"dAL, current_cache\". Outputs: \"grads[\"dAL-1\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    ### EMPIEZE EL CÓDIGO AQUÍ ### (2 líneas de código)\n",
    "    current_cache = \n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = \n",
    "    ### TERMINE EL CÓDIGO AQUÍ ###\n",
    "    \n",
    "    #Loop from l=L-2 to l=0\n",
    "    for l in reversed(range(L-1)):\n",
    "        # Gradientes para la l-ésima capa: gradientes (RELU -> LINEAL).\n",
    "        # Inputs: \"grads[\"dA\" + str(l + 1)], current_cache\". Outputs: \"grads[\"dA\" + str(l)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
    "        ### EMPIEZE EL CÓDIGO AQUÍ ### (5 líneas de código)\n",
    "        current_cache = \n",
    "        dA_prev_temp, dW_temp, db_temp = \n",
    "        grads[\"dA\" + str(l)] = \n",
    "        grads[\"dW\" + str(l + 1)] = \n",
    "        grads[\"db\" + str(l + 1)] = \n",
    "        ### TERMINE EL CÓDIGO AQUÍ ###\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uVMATlo-RBPf"
   },
   "outputs": [],
   "source": [
    "AL, Y_assess, caches = L_model_backward_test_case()\n",
    "grads = L_model_backward(AL, Y_assess, caches)\n",
    "print_grads(grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Kdvp0lKRBPh"
   },
   "source": [
    "**Salida esperada**\n",
    "\n",
    "<table style=\"width:60%\">\n",
    "  \n",
    "  <tr>\n",
    "    <td > dW1 </td> \n",
    "           <td > [[ 0.41010002  0.07807203  0.13798444  0.10502167]\n",
    " [ 0.          0.          0.          0.        ]\n",
    " [ 0.05283652  0.01005865  0.01777766  0.0135308 ]] </td> \n",
    "  </tr> \n",
    "  \n",
    "    <tr>\n",
    "    <td > db1 </td> \n",
    "           <td > [[-0.22007063]\n",
    " [ 0.        ]\n",
    " [-0.02835349]] </td> \n",
    "  </tr> \n",
    "  \n",
    "  <tr>\n",
    "  <td > dA1 </td> \n",
    "           <td > [[ 0.12913162 -0.44014127]\n",
    " [-0.14175655  0.48317296]\n",
    " [ 0.01663708 -0.05670698]] </td> \n",
    "\n",
    "  </tr> \n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "smUgcGBFRBPi"
   },
   "source": [
    "### 6.4 - Actualización de parámetros\n",
    "\n",
    "En esta sección, se actualizan los parámetros del modelo, utilizando el método de Descenso en la Dirección del Gradiente (GD): \n",
    "\n",
    "$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]} \\tag{16}$$\n",
    "$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]} \\tag{17}$$\n",
    "\n",
    "donde $\\alpha$ es la tasa de aprendizaje. Tras la actualización de los parametros, se deben guardar en los parametros del diccionario. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6cCOsI_iRBPj"
   },
   "source": [
    "**Ejercicio**: Implemente `update_parameters()` para actualizar los parámtros usando GD.\n",
    "\n",
    "**Instrucciones**:\n",
    "Actualice los parámetros utilizando GD en cada $W^{[l]}$ y $b^{[l]}$; $l = 1, 2, ..., L$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5sdfme4QRBPj"
   },
   "outputs": [],
   "source": [
    "# FUNCIÓN A CALIFICAR: update_parameters\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Actualice los parametros utilizando GD\n",
    "    Input: \n",
    "    parameters: diccionario python con los parametros \n",
    "    grads: diccionario python con los gradientes, resultado de L_model_backward\n",
    "    Output:\n",
    "    parameters: diccionario python con los parametros actualizados \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # numero de capas en la red neuronal\n",
    "\n",
    "    # Regla de actualización para cada parámetro (utilice on bucle for).\n",
    "    ### EMPIEZE EL CÓDIGO AQUÍ ### (≈ 3 líneas de código)\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = \n",
    "        parameters[\"b\" + str(l+1)] = \n",
    "    ### TERMINE EL CÓDIGO AQUÍ ###\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cHMe7PwCRBPm"
   },
   "outputs": [],
   "source": [
    "parameters, grads = update_parameters_test_case()\n",
    "parameters = update_parameters(parameters, grads, 0.1)\n",
    "\n",
    "print (\"W1 = \"+ str(parameters[\"W1\"]))\n",
    "print (\"b1 = \"+ str(parameters[\"b1\"]))\n",
    "print (\"W2 = \"+ str(parameters[\"W2\"]))\n",
    "print (\"b2 = \"+ str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XWNa34YvRBPo"
   },
   "source": [
    "**Salida esperada**\n",
    "\n",
    "<table style=\"width:100%\"> \n",
    "    <tr>\n",
    "    <td > W1 </td> \n",
    "           <td > [[-0.59562069 -0.09991781 -2.14584584  1.82662008]\n",
    " [-1.76569676 -0.80627147  0.51115557 -1.18258802]\n",
    " [-1.0535704  -0.86128581  0.68284052  2.20374577]] </td> \n",
    "  </tr> \n",
    "  \n",
    "    <tr>\n",
    "    <td > b1 </td> \n",
    "           <td > [[-0.04659241]\n",
    " [-1.28888275]\n",
    " [ 0.53405496]] </td> \n",
    "  </tr> \n",
    "  <tr>\n",
    "    <td > W2 </td> \n",
    "           <td > [[-0.55569196  0.0354055   1.32964895]]</td> \n",
    "  </tr> \n",
    "  \n",
    "    <tr>\n",
    "    <td > b2 </td> \n",
    "           <td > [[-0.84610769]] </td> \n",
    "  </tr> \n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nepZzCp7RBPq"
   },
   "source": [
    "\n",
    "## 7 - Conclusión\n",
    "\n",
    "Con este taller ya ha consturido todas las funciones requeridas para construir una red neuronal profunda! \n",
    "\n",
    "Una vez logrado esto, el siguiente taller será más directo. En el próximo taller va a construir dos modelos:\n",
    "- Una red nueronal de dos capas\n",
    "- Una red neuronal de L capas"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Taller4_RedesNeuronalesProfundas.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
